{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "104370d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a3101859",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/10/06 02:39:26 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .master(\"spark://spark-master:7077\") \\\n",
    "    .appName(\"Step5-Parquet\") \\\n",
    "    .config(\"spark.sql.autoBroadcastJoinThreshold\", -1) \\\n",
    "    .config('spark.driver.memory', '512M')  \\\n",
    "    .config('spark.executor.memory', '512M')  \\\n",
    "    .config('spark.executor.instances', '1')  \\\n",
    "    .config('spark.executor.cores', '1')  \\\n",
    "    .config(\"spark.cores.max\", \"1\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "efaff158",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 4.2M\n",
      "-rwxrwxrwx 1 root root   23K Aug 20  2021 cars.csv\n",
      "-rwxrwxrwx 1 root root   12K Aug 19  2021 Case.csv\n",
      "drwxrwxrwx 1 root root  4.0K Oct  5 18:07 datasets\n",
      "-rwxrwxrwx 1 root root  2.2K Aug 19  2021 deniro.csv\n",
      "drwxrwxrwx 1 root root  4.0K Oct  5 18:07 graph\n",
      "drwxrwxrwx 1 root root  4.0K Oct  5 18:07 logs\n",
      "-rwxrwxrwx 1 root root  1.6M Sep 12  2021 MTA-Bus-Time_.2014-08-01.txt\n",
      "-rwxrwxrwx 1 root root  227K Aug 19  2021 news.txt\n",
      "-rwxrwxrwx 1 root root  1.3M Sep 11  2021 products.parquet\n",
      "-rwxrwxrwx 1 root root   19K Aug 19  2021 Region.csv\n",
      "-rwxrwxrwx 1 root root 1023K Sep 11  2021 sales.parquet\n",
      "-rwxrwxrwx 1 root root  4.7K Sep 11  2021 sellers.parquet\n",
      "-rwxrwxrwx 1 root root   90K Aug 19  2021 TimeProvince.csv\n",
      "drwxrwxrwx 1 root root  4.0K Oct  5 18:07 trades\n",
      "-rwxrwxrwx 1 root root  1.2K Oct  4  2021 user.json\n"
     ]
    }
   ],
   "source": [
    "! ls -lh /spark-data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e479a5e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "products_table = spark.read.parquet(\"/spark-data/products.parquet\")\n",
    "sales_table = spark.read.parquet(\"/spark-data/sales.parquet\")\n",
    "sellers_table = spark.read.parquet(\"/spark-data/sellers.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "94a4efe0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 4:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+-----+--------------------+\n",
      "|product_id|product_name|price|              labels|\n",
      "+----------+------------+-----+--------------------+\n",
      "|         1|    p_000001|   30|         speak|model|\n",
      "|         2|    p_000002|   37|piece|peace|per|p...|\n",
      "|         3|    p_000003|  128|             on|fast|\n",
      "|         4|    p_000004|  145|                hard|\n",
      "|         5|    p_000005|   44|around|game|stand...|\n",
      "|         6|    p_000006|   53|                NULL|\n",
      "|         7|    p_000007|  104|  north|nature|great|\n",
      "|         8|    p_000008|   24|budget|clear|audi...|\n",
      "|         9|    p_000009|   14|             manager|\n",
      "|        10|    p_000010|   72|                NULL|\n",
      "+----------+------------+-----+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "products_table.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2eb01bc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------------+------------+\n",
      "|seller_id|       seller_name|daily_target|\n",
      "+---------+------------------+------------+\n",
      "|        0|      Chad Osborne|      250000|\n",
      "|        1|      Donald Lewis|      900423|\n",
      "|        2|      Jason Brewer|      540176|\n",
      "|        3|       Joshua Hall|     1738987|\n",
      "|        4|     Anthony Clark|     1801017|\n",
      "|        5|  Christopher Cole|      205667|\n",
      "|        6|     Crystal Kelly|      128394|\n",
      "|        7|Christopher Turner|      300273|\n",
      "|        8|         Jill Vega|      627388|\n",
      "|        9|         Susan Ray|     1947028|\n",
      "+---------+------------------+------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sellers_table.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a0adac43",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+---------+----------+---------------+--------------------+\n",
      "|order_id|product_id|seller_id|      date|num_pieces_sold|             comment|\n",
      "+--------+----------+---------+----------+---------------+--------------------+\n",
      "|       1|       891|      543|2023-05-10|             82|Ball learn east b...|\n",
      "|       2|       487|      827|2023-09-07|             54|Send animal arm m...|\n",
      "|       3|       729|       67|2023-08-20|             51|Five skill nothin...|\n",
      "|       4|       833|      484|2023-03-12|              4|Minute itself lif...|\n",
      "|       5|       325|      371|2023-10-12|             13|Certainly draw ha...|\n",
      "|       6|       127|       48|2023-06-15|             54|White itself busi...|\n",
      "|       7|       937|      303|2023-03-15|             78|Fund scene benefi...|\n",
      "|       8|       913|      424|2023-01-26|             30|Party forget six ...|\n",
      "|       9|       517|      837|2023-02-04|             36|Yourself series m...|\n",
      "|      10|       830|      534|2023-05-31|             45|Economic many pop...|\n",
      "+--------+----------+---------+----------+---------------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sales_table.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7494f2b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 7:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+--------------+-----------------+\n",
      "|summary|       product_id|  product_name|            price|\n",
      "+-------+-----------------+--------------+-----------------+\n",
      "|  count|           100000|        100000|           100000|\n",
      "|   mean|          50000.5|          NULL|         75.43343|\n",
      "| stddev|28867.65779668774|          NULL|43.41501971754077|\n",
      "|    min|                1|product_000001|                1|\n",
      "|    max|           100000|product_100000|              150|\n",
      "+-------+-----------------+--------------+-----------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "products_table.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d63d2d52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- order_id: long (nullable = true)\n",
      " |-- product_id: long (nullable = true)\n",
      " |-- seller_id: long (nullable = true)\n",
      " |-- date: string (nullable = true)\n",
      " |-- num_pieces_sold: long (nullable = true)\n",
      " |-- comment: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sales_table.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "96293bd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Orders: 2000\n",
      "Number of sellers: 101\n",
      "Number of products: 100000\n"
     ]
    }
   ],
   "source": [
    "#   Print the number of orders\n",
    "print(\"Number of Orders: {}\".format(sales_table.count()))\n",
    "\n",
    "#   Print the number of sellers\n",
    "print(\"Number of sellers: {}\".format(sellers_table.count()))\n",
    "\n",
    "#   Print the number of products\n",
    "print(\"Number of products: {}\".format(products_table.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "15bc3f13",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/10/06 08:16:57 WARN CacheManager: Asked to cache already cached data.\n",
      "23/10/06 08:16:57 WARN CacheManager: Asked to cache already cached data.\n",
      "23/10/06 08:16:57 WARN CacheManager: Asked to cache already cached data.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[seller_id: bigint, seller_name: string, daily_target: bigint]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "products_table.cache()\n",
    "sales_table.cache()\n",
    "sellers_table.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f4fe27b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 8:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|product_id|\n",
      "+----------+\n",
      "|         1|\n",
      "|         2|\n",
      "|         3|\n",
      "|         4|\n",
      "|         5|\n",
      "+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "products_table.select(products_table.product_id).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "20d73974",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|product_id|\n",
      "+----------+\n",
      "|         1|\n",
      "|         2|\n",
      "|         3|\n",
      "|         4|\n",
      "|         5|\n",
      "|         6|\n",
      "|         7|\n",
      "|         8|\n",
      "|         9|\n",
      "|        10|\n",
      "|        11|\n",
      "|        12|\n",
      "|        13|\n",
      "|        14|\n",
      "|        15|\n",
      "|        16|\n",
      "|        17|\n",
      "|        18|\n",
      "|        19|\n",
      "|        20|\n",
      "+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "products_table.select(col('product_id')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7bd27632",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of products sold at least once\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 13:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------+\n",
      "|count(DISTINCT product_id)|\n",
      "+--------------------------+\n",
      "|                      1000|\n",
      "+--------------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#   Output how many products have been actually sold at least once\n",
    "print(\"Number of products sold at least once\")\n",
    "sales_table.agg(countDistinct(col(\"product_id\"))).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2a90dd73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Product present in more orders\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 17:>                                                         (0 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---+\n",
      "|product_id|cnt|\n",
      "+----------+---+\n",
      "|       138|134|\n",
      "|       997|133|\n",
      "|       107|126|\n",
      "|         2|126|\n",
      "|       817|125|\n",
      "|       548|125|\n",
      "|       668|125|\n",
      "|       325|124|\n",
      "|       872|124|\n",
      "|       971|124|\n",
      "+----------+---+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#   Output which is the product that has been sold in more orders\n",
    "print(\"Product present in more orders\")\n",
    "sales_table.groupBy(col(\"product_id\")).agg(\n",
    "    count(\"*\").alias(\"cnt\")).orderBy(col(\"cnt\").desc()).limit(10).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ff9ddf7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Product present in more orders\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- TakeOrderedAndProject(limit=10, orderBy=[cnt#617L DESC NULLS LAST], output=[product_id#9L,cnt#617L])\n",
      "   +- HashAggregate(keys=[product_id#9L], functions=[count(1)])\n",
      "      +- Exchange hashpartitioning(product_id#9L, 200), ENSURE_REQUIREMENTS, [plan_id=318]\n",
      "         +- HashAggregate(keys=[product_id#9L], functions=[partial_count(1)])\n",
      "            +- InMemoryTableScan [product_id#9L]\n",
      "                  +- InMemoryRelation [order_id#8L, product_id#9L, seller_id#10L, date#11, num_pieces_sold#12L, comment#13], StorageLevel(disk, memory, deserialized, 1 replicas)\n",
      "                        +- *(1) ColumnarToRow\n",
      "                           +- FileScan parquet [order_id#8L,product_id#9L,seller_id#10L,date#11,num_pieces_sold#12L,comment#13] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/spark-data/sales.parquet], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<order_id:bigint,product_id:bigint,seller_id:bigint,date:string,num_pieces_sold:bigint,comm...\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#   Output which is the product that has been sold in more orders\n",
    "print(\"Product present in more orders\")\n",
    "sales_table.groupBy(col(\"product_id\")).agg(\n",
    "    count(\"*\").alias(\"cnt\")).orderBy(col(\"cnt\").desc()).limit(10).explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a7b51f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "products_table.createOrReplaceTempView(\"products\")\n",
    "sales_table.createOrReplaceTempView(\"sales\")\n",
    "sellers_table.createOrReplaceTempView(\"sellers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bcc2f8dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|product sold|\n",
      "+------------+\n",
      "|        1000|\n",
      "+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select count(distinct product_id) as `product sold`  from sales\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "550ead41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Product present in more orders\n",
      "+----------+---+\n",
      "|product_id|cnt|\n",
      "+----------+---+\n",
      "|       376| 67|\n",
      "|       600| 71|\n",
      "|       566| 75|\n",
      "|       516| 76|\n",
      "|       587| 76|\n",
      "|       472| 76|\n",
      "|       723| 76|\n",
      "|       181| 77|\n",
      "|       157| 77|\n",
      "|       299| 77|\n",
      "+----------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Product present in more orders\")\n",
    "\n",
    "spark.sql(\"\"\"select product_id, count(*) as cnt\n",
    "            from sales\n",
    "            group by 1\n",
    "            order by 2 desc \n",
    "            limit 10\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d35c3cf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Product present in more orders\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 39:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+---+\n",
      "|product_name|cnt|\n",
      "+------------+---+\n",
      "|    p_000138|134|\n",
      "|    p_000997|133|\n",
      "|    p_000002|126|\n",
      "|    p_000107|126|\n",
      "|    p_000548|125|\n",
      "|    p_000668|125|\n",
      "|    p_000817|125|\n",
      "|    p_000325|124|\n",
      "|    p_000971|124|\n",
      "|    p_000872|124|\n",
      "|    p_000850|123|\n",
      "|    p_000089|123|\n",
      "|    p_000593|123|\n",
      "|    p_000202|123|\n",
      "|    p_000222|123|\n",
      "|    p_000652|123|\n",
      "|    p_000391|122|\n",
      "|    p_000679|122|\n",
      "|    p_000499|122|\n",
      "|    p_000875|122|\n",
      "+------------+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "print(\"Product present in more orders\")\n",
    "\n",
    "df10=spark.sql(\"\"\"select product_name, count(*) as cnt\n",
    "            from sales s inner join products p on p.product_id = s.product_id\n",
    "            group by 1 \n",
    "            having cnt >1\n",
    "            order by 2 desc \n",
    "            \"\"\")\n",
    "df10.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b379e173",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df10.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1921d2a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------+\n",
      "|product_name|count(1)|\n",
      "+------------+--------+\n",
      "|    p_000007|     111|\n",
      "|    p_000480|      79|\n",
      "|    p_000914|     104|\n",
      "|    p_000057|     101|\n",
      "|    p_000493|      87|\n",
      "|    p_000674|      79|\n",
      "|    p_000981|      99|\n",
      "|    p_000181|      77|\n",
      "|    p_000435|      96|\n",
      "|    p_000551|      79|\n",
      "+------------+--------+\n",
      "only showing top 10 rows\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "df20=spark.sql(\"\"\"select product_name, count(*)\n",
    "            from sales s right outer join products p on p.product_id = s.product_id\n",
    "            group by 1 \n",
    "            \"\"\")\n",
    "# print(df20.count())\n",
    "print(df20.show(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cca0e89a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Product present in more orders\n",
      "1000\n",
      "+------------+--------+\n",
      "|product_name|count(1)|\n",
      "+------------+--------+\n",
      "|    p_000138|     134|\n",
      "|    p_000997|     133|\n",
      "|    p_000002|     126|\n",
      "|    p_000107|     126|\n",
      "|    p_000548|     125|\n",
      "|    p_000668|     125|\n",
      "|    p_000817|     125|\n",
      "|    p_000325|     124|\n",
      "|    p_000971|     124|\n",
      "|    p_000872|     124|\n",
      "|    p_000089|     123|\n",
      "|    p_000850|     123|\n",
      "|    p_000593|     123|\n",
      "|    p_000202|     123|\n",
      "|    p_000222|     123|\n",
      "|    p_000652|     123|\n",
      "|    p_000875|     122|\n",
      "|    p_000679|     122|\n",
      "|    p_000391|     122|\n",
      "|    p_000499|     122|\n",
      "+------------+--------+\n",
      "only showing top 20 rows\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(\"Product present in more orders\")\n",
    "\n",
    "df20=spark.sql(\"\"\"select product_name, count(*)\n",
    "            from sales s right outer join products p on p.product_id = s.product_id\n",
    "            group by 1 \n",
    "            order by 2 desc \n",
    "            \"\"\")\n",
    "df20.cache()\n",
    "print(df20.count())\n",
    "print(df20.show())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c27fdd9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100000\n",
      "+--------------+--------+\n",
      "|  product_name|count(1)|\n",
      "+--------------+--------+\n",
      "|product_000227|       1|\n",
      "|product_000328|       1|\n",
      "|product_000530|       1|\n",
      "|product_000718|       1|\n",
      "|product_000900|       1|\n",
      "|product_001021|       1|\n",
      "|product_001070|       1|\n",
      "|product_001172|       1|\n",
      "|product_001362|       1|\n",
      "|product_001368|       1|\n",
      "|product_001724|       1|\n",
      "|product_001796|       1|\n",
      "|product_002124|       1|\n",
      "|product_002257|       1|\n",
      "|product_002386|       1|\n",
      "|product_002574|       1|\n",
      "|product_002693|       1|\n",
      "|product_003396|       1|\n",
      "|product_003571|       1|\n",
      "|product_003634|       1|\n",
      "+--------------+--------+\n",
      "only showing top 20 rows\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(df20.count())\n",
    "print(df20.show())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9bfe8253",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\",\"20m\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0b4b4e51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------+\n",
      "|product_name|count(1)|\n",
      "+------------+--------+\n",
      "|    p_000376|      67|\n",
      "|    p_000600|      71|\n",
      "|    p_000566|      75|\n",
      "|    p_000516|      76|\n",
      "|    p_000472|      76|\n",
      "|    p_000587|      76|\n",
      "|    p_000723|      76|\n",
      "|    p_000181|      77|\n",
      "|    p_000299|      77|\n",
      "|    p_000676|      77|\n",
      "|    p_000157|      77|\n",
      "|    p_000998|      78|\n",
      "|    p_000549|      78|\n",
      "|    p_000720|      78|\n",
      "|    p_000830|      78|\n",
      "|    p_000633|      78|\n",
      "|    p_000480|      79|\n",
      "|    p_000674|      79|\n",
      "|    p_000551|      79|\n",
      "|    p_000902|      79|\n",
      "+------------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df30=spark.sql(\"\"\"select product_name, count(*)\n",
    "            from sales s join products p on p.product_id = s.product_id\n",
    "            group by 1 \n",
    "            order by 2 asc \n",
    "            \"\"\")\n",
    "df30.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "02aa387e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------+\n",
      "|product_name|count(1)|\n",
      "+------------+--------+\n",
      "|    p_000376|      67|\n",
      "|    p_000600|      71|\n",
      "|    p_000566|      75|\n",
      "|    p_000472|      76|\n",
      "|    p_000516|      76|\n",
      "|    p_000587|      76|\n",
      "|    p_000723|      76|\n",
      "|    p_000181|      77|\n",
      "|    p_000299|      77|\n",
      "|    p_000676|      77|\n",
      "|    p_000157|      77|\n",
      "|    p_000998|      78|\n",
      "|    p_000549|      78|\n",
      "|    p_000720|      78|\n",
      "|    p_000830|      78|\n",
      "|    p_000633|      78|\n",
      "|    p_000480|      79|\n",
      "|    p_000674|      79|\n",
      "|    p_000551|      79|\n",
      "|    p_000902|      79|\n",
      "+------------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\",\"-1\")\n",
    "df40=spark.sql(\"\"\"select product_name, count(*)\n",
    "            from sales s join products p on p.product_id = s.product_id\n",
    "            group by 1 \n",
    "            order by 2 asc \n",
    "            \"\"\")\n",
    "df40.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12297ebe-81fc-41ca-a48c-800a213ea4c2",
   "metadata": {},
   "source": [
    "Certainly! In Apache Spark, BROADCASTJOIN, SHUFFLE_HASH, and SHUFFLE_MERGE are strategies used for joining data frames or datasets. Let me explain each of them with examples:\n",
    "\n",
    "### 1. **BROADCASTJOIN:**\n",
    "When one of the DataFrames is small enough to fit in the memory of each worker node, you can use a broadcast join. In this type of join, the smaller DataFrame is broadcasted to all worker nodes, and the join operation is performed locally on each node.\n",
    "\n",
    "\n",
    "\n",
    "### 2. **SHUFFLE_HASH:**\n",
    "Shuffle hash join is used when the join keys are not evenly distributed and don't fit in memory. In this approach, data is shuffled across the cluster based on the join keys. Each partition of the DataFrames with the same join key is moved to the same node, where the actual join operation is performed.\n",
    "\n",
    "\n",
    "### 3. **SHUFFLE_MERGE:**\n",
    "Shuffle merge join is an optimization introduced in Spark 3.0. It is used when both DataFrames to be joined are hash-partitioned on the join key. Instead of performing a full shuffle, this join strategy can efficiently merge the already shuffled partitions of both DataFrames.\n",
    "\n",
    "\n",
    "In these examples, `commonColumn` represents the column on which the DataFrames are being joined. The appropriate join strategy depends on the size of the DataFrames, the distribution of join keys, and the available memory in the cluster. Choosing the right join strategy is crucial for optimizing the performance of Spark applications.\n",
    "\n",
    "The choice of join strategy in Apache Spark depends on various factors such as the size of the DataFrames, the distribution of the join keys, the available memory in the cluster, and the overall cluster configuration. Here's a guideline on when to use each join strategy:\n",
    "\n",
    "### 1. **BROADCASTJOIN:**\n",
    "\n",
    "- **Use Case:** Broadcast join is ideal when one of the DataFrames is small enough to fit in the memory of each worker node.\n",
    "  \n",
    "- **When to Use:**\n",
    "  - When one DataFrame is significantly smaller than the other.\n",
    "  - When the smaller DataFrame can fit in the memory of each worker node.\n",
    "  - When network bandwidth is a concern, and minimizing data shuffling is important.\n",
    "  \n",
    "- **Example:** Joining a large DataFrame with a small lookup table.\n",
    "\n",
    "### 2. **SHUFFLE_HASH:**\n",
    "\n",
    "- **Use Case:** Shuffle hash join is suitable when join keys are not evenly distributed and do not fit in memory, but both DataFrames are not hash-partitioned on the join key.\n",
    "  \n",
    "- **When to Use:**\n",
    "  - When join keys are not evenly distributed.\n",
    "  - When DataFrames are not hash-partitioned on the join key.\n",
    "  - When the size of DataFrames is moderate and cannot fit in memory.\n",
    "  \n",
    "- **Example:** Joining two large DataFrames on a non-uniformly distributed join key.\n",
    "\n",
    "### 3. **SHUFFLE_MERGE:**\n",
    "\n",
    "- **Use Case:** Shuffle merge join is efficient when both DataFrames are hash-partitioned on the join key and can take advantage of sorted and partitioned data for the join operation.\n",
    "  \n",
    "- **When to Use:**\n",
    "  - When both DataFrames are hash-partitioned on the join key.\n",
    "  - When both DataFrames are large, and the join keys are evenly distributed.\n",
    "  - When Spark version is 3.0 or higher (as shuffle merge join is optimized in these versions).\n",
    "  \n",
    "- **Example:** Joining two large DataFrames that are already hash-partitioned on the join key.\n",
    "\n",
    "In summary, consider the size of your DataFrames, the distribution of your join keys, and the available memory in your cluster when choosing a join strategy. Always profile your data and perform benchmarking to determine the most efficient join strategy for your specific use case. Additionally, staying up-to-date with the latest optimizations and features in newer versions of Apache Spark can also help you make better decisions regarding join strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0b0d67a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "+------------+--------+\n",
      "|product_name|count(1)|\n",
      "+------------+--------+\n",
      "|    p_000376|      67|\n",
      "|    p_000600|      71|\n",
      "|    p_000566|      75|\n",
      "|    p_000472|      76|\n",
      "|    p_000516|      76|\n",
      "|    p_000587|      76|\n",
      "|    p_000723|      76|\n",
      "|    p_000181|      77|\n",
      "|    p_000299|      77|\n",
      "|    p_000676|      77|\n",
      "|    p_000157|      77|\n",
      "|    p_000998|      78|\n",
      "|    p_000549|      78|\n",
      "|    p_000720|      78|\n",
      "|    p_000830|      78|\n",
      "|    p_000633|      78|\n",
      "|    p_000480|      79|\n",
      "|    p_000674|      79|\n",
      "|    p_000551|      79|\n",
      "|    p_000902|      79|\n",
      "+------------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# SELECT /*+  BROADCASTJOIN(small) */ * \n",
    "\n",
    "df50=spark.sql(\"\"\"select /*+  BROADCASTJOIN(s) */   product_name, count(*)  \n",
    "            from products p join sales s on p.product_id = s.product_id\n",
    "            group by 1 \n",
    "            order by 2 asc \n",
    "            \"\"\")\n",
    "print(df50.count())\n",
    "df50.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5f33043c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "+------------+--------+\n",
      "|product_name|count(1)|\n",
      "+------------+--------+\n",
      "|    p_000376|      67|\n",
      "|    p_000600|      71|\n",
      "|    p_000566|      75|\n",
      "|    p_000516|      76|\n",
      "|    p_000472|      76|\n",
      "|    p_000587|      76|\n",
      "|    p_000723|      76|\n",
      "|    p_000181|      77|\n",
      "|    p_000299|      77|\n",
      "|    p_000676|      77|\n",
      "|    p_000157|      77|\n",
      "|    p_000998|      78|\n",
      "|    p_000549|      78|\n",
      "|    p_000720|      78|\n",
      "|    p_000830|      78|\n",
      "|    p_000633|      78|\n",
      "|    p_000480|      79|\n",
      "|    p_000674|      79|\n",
      "|    p_000551|      79|\n",
      "|    p_000902|      79|\n",
      "+------------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df50=spark.sql(\"\"\"select /*+  SHUFFLE_HASH(s) */   product_name, count(*)  \n",
    "            from products p join sales s on p.product_id = s.product_id\n",
    "            group by 1 \n",
    "            order by 2 asc \n",
    "            \"\"\")\n",
    "print(df50.count())\n",
    "df50.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d230cb86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "+------------+--------+\n",
      "|product_name|count(1)|\n",
      "+------------+--------+\n",
      "|    p_000376|      67|\n",
      "|    p_000600|      71|\n",
      "|    p_000566|      75|\n",
      "|    p_000472|      76|\n",
      "|    p_000516|      76|\n",
      "|    p_000587|      76|\n",
      "|    p_000723|      76|\n",
      "|    p_000181|      77|\n",
      "|    p_000299|      77|\n",
      "|    p_000676|      77|\n",
      "|    p_000157|      77|\n",
      "|    p_000998|      78|\n",
      "|    p_000549|      78|\n",
      "|    p_000720|      78|\n",
      "|    p_000830|      78|\n",
      "|    p_000633|      78|\n",
      "|    p_000480|      79|\n",
      "|    p_000674|      79|\n",
      "|    p_000551|      79|\n",
      "|    p_000902|      79|\n",
      "+------------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df50=spark.sql(\"\"\"select /*+  SHUFFLE_MERGE(s) */   product_name, count(*)  \n",
    "            from products p join sales s on p.product_id = s.product_id\n",
    "            group by 1 \n",
    "            order by 2 asc \n",
    "            \"\"\")\n",
    "print(df50.count())\n",
    "df50.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d9f4ec40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------+\n",
      "|seller_id|          avg(ratio)|\n",
      "+---------+--------------------+\n",
      "|       29|4.644988939825174...|\n",
      "|       26|2.319554387605397...|\n",
      "|       65|2.931429678805068...|\n",
      "|       54|2.389661293535090...|\n",
      "|       19|2.858258703574406...|\n",
      "|       22|3.708259605141521E-5|\n",
      "|        7|1.555967948441298...|\n",
      "|       77|2.981032682717429...|\n",
      "|       34|3.477284846496053...|\n",
      "|       50|2.870018587854760...|\n",
      "|       94|4.674046551971184E-5|\n",
      "|       57|3.405221429066354E-5|\n",
      "|       32|8.634325709528986E-5|\n",
      "|       43|2.882534893949249...|\n",
      "|       84|4.044736739416897...|\n",
      "|       31|3.006195752651825...|\n",
      "|       98|6.342958088068233E-5|\n",
      "|       39|5.703705240055282...|\n",
      "|       25|3.625800692872154...|\n",
      "|       95|2.924769017292419...|\n",
      "+---------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "####  Dataframe Joins\n",
    "print(sales_table.join(broadcast(sellers_table), sales_table[\"seller_id\"] == sellers_table[\"seller_id\"], \"inner\").withColumn(\n",
    "    \"ratio\", sales_table[\"num_pieces_sold\"]/sellers_table[\"daily_target\"]\n",
    ").groupBy(sales_table[\"seller_id\"]).agg(avg(\"ratio\")).show())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6131f31c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 158:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+---------+----------+---------------+--------------------+--------------------+\n",
      "|order_id|product_id|seller_id|      date|num_pieces_sold|       bill_raw_text|         hashed_bill|\n",
      "+--------+----------+---------+----------+---------------+--------------------+--------------------+\n",
      "|       1|     57164|       69|2021-10-04|             20|tvuerrnvlBwhejtyz...|76c669d6d3e9c8ddd...|\n",
      "|       2|     17466|        4|2021-10-03|             80|ezoexpsddxnahgkxr...|ezoexpsddxnahgkxr...|\n",
      "|       3|     25747|       81|2021-10-08|             24|rrzavxtphkxhyiicu...|11d2ead66af66ec03...|\n",
      "|       4|     53745|       59|2021-10-02|             92|jklcqmgjbfqxtsugt...|jklcqmgjbfqxtsugt...|\n",
      "|       5|     31885|        5|2021-10-06|             10|azdujdzlhddjebywm...|4ac76798d905e2ba4...|\n",
      "|       6|     36014|       52|2021-10-08|             71|jpjwwfckwolwphpkq...|63f8773c95665bb5e...|\n",
      "|       7|      1709|       59|2021-10-05|              3|gcjrymmjlcmkhoyxq...|96a5d3cfb3d834fa1...|\n",
      "|       8|      1736|       98|2021-10-04|             87|ukhygpejttpbwwden...|ukhygpejttpbwwden...|\n",
      "|       9|     58029|       32|2021-10-04|             10|uzfdkgybsrraxpspc...|2d77cd280ad3b110d...|\n",
      "|      10|     46330|       44|2021-10-01|             92|hqteqmecidiwdfosn...|hqteqmecidiwdfosn...|\n",
      "|      11|     25341|        9|2021-10-02|             74|qzqultdjscdxhugvq...|440a4c004230c125d...|\n",
      "|      12|     41701|       63|2021-10-08|             39|zhkzrjhamzjozqjfb...|zhkzrjhamzjozqjfb...|\n",
      "|      13|      1115|       29|2021-10-10|             17|nrjioabfbltimreey...|318f819c45b590332...|\n",
      "|      14|     60704|        5|2021-10-03|             71|aszKqnhkzwfokbgcc...|e9d086c34ce297261...|\n",
      "|      15|     21436|       39|2021-10-09|             27|clfzmglinsfchjcsq...|0e19a776dcbb8c8ac...|\n",
      "|      16|     42125|       28|2021-10-02|             66|wrpmmxfckktvgyure...|wrpmmxfckktvgyure...|\n",
      "|      17|     71664|       52|2021-10-10|             28|ctmxoxxrdmvwcpaqx...|3c88e0b8f263766d0...|\n",
      "|      18|     80908|       81|2021-10-01|             93|opczxzegrqpvlwgpt...|opczxzegrqpvlwgpt...|\n",
      "|      19|     68262|       25|2021-10-03|              9|acgjblasaowckpprv...|acf476721172c534d...|\n",
      "|      20|     46309|       23|2021-10-05|             67|jijgovcmwrzbyxagj...|jijgovcmwrzbyxagj...|\n",
      "+--------+----------+---------+----------+---------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/10/04 22:05:21 ERROR StandaloneSchedulerBackend: Application has been killed. Reason: Master removed our application: KILLED\n",
      "23/10/04 22:05:30 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exiting due to error from cluster scheduler: Master removed our application: KILLED\n",
      "\tat org.apache.spark.errors.SparkCoreErrors$.clusterSchedulerError(SparkCoreErrors.scala:291)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.error(TaskSchedulerImpl.scala:981)\n",
      "\tat org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend.dead(StandaloneSchedulerBackend.scala:165)\n",
      "\tat org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint.markDead(StandaloneAppClient.scala:263)\n",
      "\tat org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint$$anonfun$receive$1.applyOrElse(StandaloneAppClient.scala:170)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:115)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import Row, Window\n",
    "from pyspark.sql.types import IntegerType\n",
    "import hashlib\n",
    "\n",
    "#   Define the UDF function\n",
    "def algo(order_id, bill_text):\n",
    "    #   If number is even\n",
    "    ret = bill_text.encode(\"utf-8\")\n",
    "    if int(order_id) % 2 == 0:\n",
    "        #   Count number of 'A'\n",
    "        cnt_A = bill_text.count(\"A\")\n",
    "        for _c in range(0, cnt_A):\n",
    "            ret = hashlib.md5(ret).hexdigest().encode(\"utf-8\")\n",
    "        ret = ret.decode('utf-8')\n",
    "    else:\n",
    "        ret = hashlib.sha256(ret).hexdigest()\n",
    "    return ret\n",
    "\n",
    "#   Register the UDF function.\n",
    "algo_udf = spark.udf.register(\"algo\", algo)\n",
    "\n",
    "sales_table = sales_table.withColumn(\"hashed_bill\", algo_udf(col(\"order_id\"), col(\"bill_raw_text\")))\n",
    "\n",
    "sales_table.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2e6316e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d2480d1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
